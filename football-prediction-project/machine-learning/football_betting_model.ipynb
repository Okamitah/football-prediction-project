{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Football betting model**"
      ],
      "metadata": {
        "id": "jR3JCMn2A-6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll try to train a *neural network* to find **value bets**."
      ],
      "metadata": {
        "id": "ZQ7l-hmvJaYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Value bets are just bets where we have an advantage over the bookie. It is when the **provided odds** are lower than the **actual odds**.\n"
      ],
      "metadata": {
        "id": "9h-r9XQGJ0LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's try to find them!"
      ],
      "metadata": {
        "id": "ksTsn8cVKF2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Importing necessary files and libraries**"
      ],
      "metadata": {
        "id": "seq_x4ssKL6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "GrZgejbn9a8m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our files\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "files = [f for f in uploaded.keys() if f.endswith('.csv')]\n",
        "data_frames = [pd.read_csv(f) for f in files]\n",
        "full_data = pd.concat(data_frames, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H2aLnGME9dx8",
        "outputId": "5848ff54-82cb-46da-dfe0-d12812bac2d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-743805ea-5d17-4644-84b8-9ac21676fcbe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-743805ea-5d17-4644-84b8-9ac21676fcbe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bu20_ratings.csv to bu20_ratings (2).csv\n",
            "Saving bu21_ratings.csv to bu21_ratings (2).csv\n",
            "Saving bu22_ratings.csv to bu22_ratings (2).csv\n",
            "Saving bu23_ratings.csv to bu23_ratings (2).csv\n",
            "Saving bu24_ratings.csv to bu24_ratings (2).csv\n",
            "Saving l121_ratings.csv to l121_ratings (2).csv\n",
            "Saving l122_ratings.csv to l122_ratings (2).csv\n",
            "Saving l123_ratings.csv to l123_ratings (2).csv\n",
            "Saving l124_ratings.csv to l124_ratings (2).csv\n",
            "Saving li20_ratings.csv to li20_ratings (2).csv\n",
            "Saving li21_ratings.csv to li21_ratings (2).csv\n",
            "Saving li22_ratings.csv to li22_ratings (2).csv\n",
            "Saving li23_ratings.csv to li23_ratings (2).csv\n",
            "Saving li24_ratings.csv to li24_ratings (2).csv\n",
            "Saving pl20_ratings.csv to pl20_ratings (2).csv\n",
            "Saving pl21_ratings.csv to pl21_ratings (2).csv\n",
            "Saving pl22_ratings.csv to pl22_ratings (2).csv\n",
            "Saving pl23_ratings.csv to pl23_ratings (2).csv\n",
            "Saving pl24_ratings.csv to pl24_ratings (2).csv\n",
            "Saving pr20_ratings.csv to pr20_ratings (2).csv\n",
            "Saving pr21_ratings.csv to pr21_ratings (2).csv\n",
            "Saving pr22_ratings.csv to pr22_ratings (2).csv\n",
            "Saving pr23_ratings.csv to pr23_ratings (2).csv\n",
            "Saving pr24_ratings.csv to pr24_ratings (2).csv\n",
            "Saving sa20_ratings.csv to sa20_ratings (2).csv\n",
            "Saving sa21_ratings.csv to sa21_ratings (2).csv\n",
            "Saving sa22_ratings.csv to sa22_ratings (2).csv\n",
            "Saving sa23_ratings.csv to sa23_ratings (2).csv\n",
            "Saving sa24_ratings.csv to sa24_ratings (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Organising and normalizing our features**"
      ],
      "metadata": {
        "id": "FbA85XBcKW0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll add the rank difference, rating difference and result features (home - away) to make it one column insteadof two and normalize them\n",
        "\n",
        "full_data['Rank Difference'] = full_data['Home ranking'] - full_data['Away ranking']\n",
        "full_data['Rank Difference'] = (full_data['Rank Difference'] - full_data['Rank Difference'].mean()) / full_data['Rank Difference'].std()\n",
        "\n",
        "full_data['Rating Difference'] = full_data['Home Last Avg Rating'] - full_data['Away Last Avg Rating']\n",
        "full_data['Rating Difference'] = (full_data['Rating Difference'] - full_data['Rating Difference'].mean()) / full_data['Rating Difference'].std()\n",
        "\n",
        "full_data['Results'] = full_data['Home score'] - full_data['Away score']\n",
        "full_data['Label'] = full_data['Results'].apply(lambda x: 1 if x > 0 else (0 if x == 0 else 2))\n",
        "full_data['Results'] = (full_data['Results'] - full_data['Results'].mean()) / full_data['Results'].std()\n",
        "odds_features = ['Home odds', 'Draw odds','Away odds']\n",
        "odds = full_data[odds_features].values\n",
        "\n",
        "# And make our tensors\n",
        "\n",
        "features = ['Rank Difference', 'Rating Difference', 'Home odds', 'Draw odds', 'Away odds']\n",
        "\n",
        "X = full_data[features].values\n",
        "y = full_data['Label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test, odds_train, odds_test = train_test_split(X, y, odds, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "odds_train_tensor = torch.tensor(odds_train, dtype=torch.float32)\n",
        "odds_test_tensor = torch.tensor(odds_test, dtype=torch.float32)\n",
        "\n",
        "# Move tensors to GPU if available\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "X_train_tensor = X_train_tensor.to(device)\n",
        "y_train_tensor = y_train_tensor.to(device)\n",
        "X_test_tensor = X_test_tensor.to(device)\n",
        "y_test_tensor = y_test_tensor.to(device)\n",
        "odds_train_tensor = odds_train_tensor.to(device)\n",
        "odds_test_tensor = odds_test_tensor.to(device)"
      ],
      "metadata": {
        "id": "T0TJzIuN-PVS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll make our Dataset class (to ensure our structure conformity)\n",
        "\n",
        "class FootballDataset(Dataset):\n",
        "    def __init__(self, X, y, odds):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.odds = odds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.odds[idx]\n",
        "\n",
        "\n",
        "train_dataset = FootballDataset(X_train_tensor, y_train_tensor, odds_train_tensor)\n",
        "test_dataset = FootballDataset(X_test_tensor, y_test_tensor, odds_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "A5vrRab5-oS5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Making our neural network**"
      ],
      "metadata": {
        "id": "LZ_Q1XnEKoGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L5IGfLZRA6LA"
      },
      "outputs": [],
      "source": [
        "# And make our model\n",
        "\n",
        "class BettingNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(BettingNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BettingNN(input_size=5, num_classes=3).to(device)"
      ],
      "metadata": {
        "id": "xiJGl6nzzdr4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll customise our **loss function**, because we're trying to find value bets, not just predict the outcome, because we can find a value bet and it'll lose, so we have to take that into consideration."
      ],
      "metadata": {
        "id": "pUUONGVgKzh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_loss_function(predictions, labels, odds):\n",
        "    probabilities = torch.softmax(predictions, dim=1)\n",
        "\n",
        "    batch_size = labels.size(0)\n",
        "    ev = probabilities * odds\n",
        "\n",
        "    actual_ev = ev.gather(1, labels.unsqueeze(1)).squeeze()\n",
        "    loss = -actual_ev.mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train_model(model, train_loader, optimizer, num_epochs):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels, odds in train_loader:\n",
        "\n",
        "            inputs, labels, odds = inputs.to(device), labels.to(device), odds.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = custom_loss_function(outputs, labels, odds)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "def evaluate_profitability(model, test_loader):\n",
        "    model.eval()\n",
        "    total_bets = 0\n",
        "    total_profit = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, odds in test_loader:\n",
        "            inputs, odds = inputs.to(device), odds.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predicted_outcomes = probabilities.argmax(dim=1)\n",
        "            for i in range(inputs.size(0)):\n",
        "                pred = predicted_outcomes[i].item()\n",
        "                if pred == labels[i].item():\n",
        "                    profit = odds[i, pred] - 1\n",
        "                else:\n",
        "                    profit = -1\n",
        "                total_profit += profit\n",
        "                total_bets += 1\n",
        "    roi = (total_profit / total_bets) if total_bets > 0 else 0\n",
        "    print(f\"Total Bets: {total_bets}, Total Profit: {total_profit:.2f}, ROI: {roi * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "hsI3cWQkza_i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "MlCgwQsezEo-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Training our neural network**"
      ],
      "metadata": {
        "id": "LQc6KHaWLNq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10000\n",
        "train_model(model, train_loader, optimizer, num_epochs)\n",
        "evaluate_profitability(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ocRgDvv-yRN",
        "outputId": "3ebc2fc9-ac4e-49c8-a0c2-80b02e7bf99e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10000, Loss: -1.8725\n",
            "Epoch 20/10000, Loss: -1.9157\n",
            "Epoch 30/10000, Loss: -1.9300\n",
            "Epoch 40/10000, Loss: -1.9415\n",
            "Epoch 50/10000, Loss: -1.9462\n",
            "Epoch 60/10000, Loss: -1.9495\n",
            "Epoch 70/10000, Loss: -1.9512\n",
            "Epoch 80/10000, Loss: -1.9524\n",
            "Epoch 90/10000, Loss: -1.9545\n",
            "Epoch 100/10000, Loss: -1.9554\n",
            "Epoch 110/10000, Loss: -1.9568\n",
            "Epoch 120/10000, Loss: -1.9573\n",
            "Epoch 130/10000, Loss: -1.9582\n",
            "Epoch 140/10000, Loss: -1.9592\n",
            "Epoch 150/10000, Loss: -1.9603\n",
            "Epoch 160/10000, Loss: -1.9608\n",
            "Epoch 170/10000, Loss: -1.9611\n",
            "Epoch 180/10000, Loss: -1.9618\n",
            "Epoch 190/10000, Loss: -1.9618\n",
            "Epoch 200/10000, Loss: -1.9626\n",
            "Epoch 210/10000, Loss: -1.9629\n",
            "Epoch 220/10000, Loss: -1.9628\n",
            "Epoch 230/10000, Loss: -1.9628\n",
            "Epoch 240/10000, Loss: -1.9635\n",
            "Epoch 250/10000, Loss: -1.9643\n",
            "Epoch 260/10000, Loss: -1.9646\n",
            "Epoch 270/10000, Loss: -1.9645\n",
            "Epoch 280/10000, Loss: -1.9649\n",
            "Epoch 290/10000, Loss: -1.9654\n",
            "Epoch 300/10000, Loss: -1.9655\n",
            "Epoch 310/10000, Loss: -1.9655\n",
            "Epoch 320/10000, Loss: -1.9664\n",
            "Epoch 330/10000, Loss: -1.9672\n",
            "Epoch 340/10000, Loss: -1.9666\n",
            "Epoch 350/10000, Loss: -1.9674\n",
            "Epoch 360/10000, Loss: -1.9672\n",
            "Epoch 370/10000, Loss: -1.9684\n",
            "Epoch 380/10000, Loss: -1.9681\n",
            "Epoch 390/10000, Loss: -1.9687\n",
            "Epoch 400/10000, Loss: -1.9682\n",
            "Epoch 410/10000, Loss: -1.9697\n",
            "Epoch 420/10000, Loss: -1.9698\n",
            "Epoch 430/10000, Loss: -1.9704\n",
            "Epoch 440/10000, Loss: -1.9702\n",
            "Epoch 450/10000, Loss: -1.9712\n",
            "Epoch 460/10000, Loss: -1.9713\n",
            "Epoch 470/10000, Loss: -1.9720\n",
            "Epoch 480/10000, Loss: -1.9722\n",
            "Epoch 490/10000, Loss: -1.9730\n",
            "Epoch 500/10000, Loss: -1.9725\n",
            "Epoch 510/10000, Loss: -1.9729\n",
            "Epoch 520/10000, Loss: -1.9736\n",
            "Epoch 530/10000, Loss: -1.9733\n",
            "Epoch 540/10000, Loss: -1.9741\n",
            "Epoch 550/10000, Loss: -1.9744\n",
            "Epoch 560/10000, Loss: -1.9745\n",
            "Epoch 570/10000, Loss: -1.9751\n",
            "Epoch 580/10000, Loss: -1.9751\n",
            "Epoch 590/10000, Loss: -1.9756\n",
            "Epoch 600/10000, Loss: -1.9754\n",
            "Epoch 610/10000, Loss: -1.9759\n",
            "Epoch 620/10000, Loss: -1.9760\n",
            "Epoch 630/10000, Loss: -1.9762\n",
            "Epoch 640/10000, Loss: -1.9767\n",
            "Epoch 650/10000, Loss: -1.9770\n",
            "Epoch 660/10000, Loss: -1.9777\n",
            "Epoch 670/10000, Loss: -1.9775\n",
            "Epoch 680/10000, Loss: -1.9770\n",
            "Epoch 690/10000, Loss: -1.9778\n",
            "Epoch 700/10000, Loss: -1.9781\n",
            "Epoch 710/10000, Loss: -1.9780\n",
            "Epoch 720/10000, Loss: -1.9780\n",
            "Epoch 730/10000, Loss: -1.9785\n",
            "Epoch 740/10000, Loss: -1.9788\n",
            "Epoch 750/10000, Loss: -1.9784\n",
            "Epoch 760/10000, Loss: -1.9789\n",
            "Epoch 770/10000, Loss: -1.9788\n",
            "Epoch 780/10000, Loss: -1.9796\n",
            "Epoch 790/10000, Loss: -1.9796\n",
            "Epoch 800/10000, Loss: -1.9793\n",
            "Epoch 810/10000, Loss: -1.9794\n",
            "Epoch 820/10000, Loss: -1.9802\n",
            "Epoch 830/10000, Loss: -1.9798\n",
            "Epoch 840/10000, Loss: -1.9806\n",
            "Epoch 850/10000, Loss: -1.9805\n",
            "Epoch 860/10000, Loss: -1.9806\n",
            "Epoch 870/10000, Loss: -1.9808\n",
            "Epoch 880/10000, Loss: -1.9809\n",
            "Epoch 890/10000, Loss: -1.9812\n",
            "Epoch 900/10000, Loss: -1.9809\n",
            "Epoch 910/10000, Loss: -1.9807\n",
            "Epoch 920/10000, Loss: -1.9811\n",
            "Epoch 930/10000, Loss: -1.9817\n",
            "Epoch 940/10000, Loss: -1.9809\n",
            "Epoch 950/10000, Loss: -1.9818\n",
            "Epoch 960/10000, Loss: -1.9823\n",
            "Epoch 970/10000, Loss: -1.9822\n",
            "Epoch 980/10000, Loss: -1.9819\n",
            "Epoch 990/10000, Loss: -1.9828\n",
            "Epoch 1000/10000, Loss: -1.9828\n",
            "Epoch 1010/10000, Loss: -1.9821\n",
            "Epoch 1020/10000, Loss: -1.9825\n",
            "Epoch 1030/10000, Loss: -1.9828\n",
            "Epoch 1040/10000, Loss: -1.9831\n",
            "Epoch 1050/10000, Loss: -1.9826\n",
            "Epoch 1060/10000, Loss: -1.9839\n",
            "Epoch 1070/10000, Loss: -1.9835\n",
            "Epoch 1080/10000, Loss: -1.9835\n",
            "Epoch 1090/10000, Loss: -1.9839\n",
            "Epoch 1100/10000, Loss: -1.9839\n",
            "Epoch 1110/10000, Loss: -1.9839\n",
            "Epoch 1120/10000, Loss: -1.9830\n",
            "Epoch 1130/10000, Loss: -1.9842\n",
            "Epoch 1140/10000, Loss: -1.9840\n",
            "Epoch 1150/10000, Loss: -1.9846\n",
            "Epoch 1160/10000, Loss: -1.9848\n",
            "Epoch 1170/10000, Loss: -1.9841\n",
            "Epoch 1180/10000, Loss: -1.9845\n",
            "Epoch 1190/10000, Loss: -1.9842\n",
            "Epoch 1200/10000, Loss: -1.9840\n",
            "Epoch 1210/10000, Loss: -1.9853\n",
            "Epoch 1220/10000, Loss: -1.9849\n",
            "Epoch 1230/10000, Loss: -1.9856\n",
            "Epoch 1240/10000, Loss: -1.9857\n",
            "Epoch 1250/10000, Loss: -1.9854\n",
            "Epoch 1260/10000, Loss: -1.9853\n",
            "Epoch 1270/10000, Loss: -1.9861\n",
            "Epoch 1280/10000, Loss: -1.9854\n",
            "Epoch 1290/10000, Loss: -1.9857\n",
            "Epoch 1300/10000, Loss: -1.9857\n",
            "Epoch 1310/10000, Loss: -1.9863\n",
            "Epoch 1320/10000, Loss: -1.9857\n",
            "Epoch 1330/10000, Loss: -1.9858\n",
            "Epoch 1340/10000, Loss: -1.9856\n",
            "Epoch 1350/10000, Loss: -1.9861\n",
            "Epoch 1360/10000, Loss: -1.9851\n",
            "Epoch 1370/10000, Loss: -1.9864\n",
            "Epoch 1380/10000, Loss: -1.9864\n",
            "Epoch 1390/10000, Loss: -1.9860\n",
            "Epoch 1400/10000, Loss: -1.9868\n",
            "Epoch 1410/10000, Loss: -1.9862\n",
            "Epoch 1420/10000, Loss: -1.9864\n",
            "Epoch 1430/10000, Loss: -1.9869\n",
            "Epoch 1440/10000, Loss: -1.9869\n",
            "Epoch 1450/10000, Loss: -1.9873\n",
            "Epoch 1460/10000, Loss: -1.9868\n",
            "Epoch 1470/10000, Loss: -1.9870\n",
            "Epoch 1480/10000, Loss: -1.9872\n",
            "Epoch 1490/10000, Loss: -1.9863\n",
            "Epoch 1500/10000, Loss: -1.9872\n",
            "Epoch 1510/10000, Loss: -1.9877\n",
            "Epoch 1520/10000, Loss: -1.9876\n",
            "Epoch 1530/10000, Loss: -1.9881\n",
            "Epoch 1540/10000, Loss: -1.9877\n",
            "Epoch 1550/10000, Loss: -1.9872\n",
            "Epoch 1560/10000, Loss: -1.9873\n",
            "Epoch 1570/10000, Loss: -1.9878\n",
            "Epoch 1580/10000, Loss: -1.9874\n",
            "Epoch 1590/10000, Loss: -1.9877\n",
            "Epoch 1600/10000, Loss: -1.9869\n",
            "Epoch 1610/10000, Loss: -1.9883\n",
            "Epoch 1620/10000, Loss: -1.9878\n",
            "Epoch 1630/10000, Loss: -1.9884\n",
            "Epoch 1640/10000, Loss: -1.9874\n",
            "Epoch 1650/10000, Loss: -1.9883\n",
            "Epoch 1660/10000, Loss: -1.9882\n",
            "Epoch 1670/10000, Loss: -1.9888\n",
            "Epoch 1680/10000, Loss: -1.9888\n",
            "Epoch 1690/10000, Loss: -1.9882\n",
            "Epoch 1700/10000, Loss: -1.9890\n",
            "Epoch 1710/10000, Loss: -1.9881\n",
            "Epoch 1720/10000, Loss: -1.9883\n",
            "Epoch 1730/10000, Loss: -1.9891\n",
            "Epoch 1740/10000, Loss: -1.9888\n",
            "Epoch 1750/10000, Loss: -1.9887\n",
            "Epoch 1760/10000, Loss: -1.9893\n",
            "Epoch 1770/10000, Loss: -1.9891\n",
            "Epoch 1780/10000, Loss: -1.9883\n",
            "Epoch 1790/10000, Loss: -1.9887\n",
            "Epoch 1800/10000, Loss: -1.9896\n",
            "Epoch 1810/10000, Loss: -1.9888\n",
            "Epoch 1820/10000, Loss: -1.9892\n",
            "Epoch 1830/10000, Loss: -1.9898\n",
            "Epoch 1840/10000, Loss: -1.9893\n",
            "Epoch 1850/10000, Loss: -1.9893\n",
            "Epoch 1860/10000, Loss: -1.9893\n",
            "Epoch 1870/10000, Loss: -1.9895\n",
            "Epoch 1880/10000, Loss: -1.9897\n",
            "Epoch 1890/10000, Loss: -1.9897\n",
            "Epoch 1900/10000, Loss: -1.9898\n",
            "Epoch 1910/10000, Loss: -1.9903\n",
            "Epoch 1920/10000, Loss: -1.9896\n",
            "Epoch 1930/10000, Loss: -1.9903\n",
            "Epoch 1940/10000, Loss: -1.9903\n",
            "Epoch 1950/10000, Loss: -1.9904\n",
            "Epoch 1960/10000, Loss: -1.9891\n",
            "Epoch 1970/10000, Loss: -1.9905\n",
            "Epoch 1980/10000, Loss: -1.9905\n",
            "Epoch 1990/10000, Loss: -1.9895\n",
            "Epoch 2000/10000, Loss: -1.9899\n",
            "Epoch 2010/10000, Loss: -1.9896\n",
            "Epoch 2020/10000, Loss: -1.9901\n",
            "Epoch 2030/10000, Loss: -1.9903\n",
            "Epoch 2040/10000, Loss: -1.9903\n",
            "Epoch 2050/10000, Loss: -1.9903\n",
            "Epoch 2060/10000, Loss: -1.9904\n",
            "Epoch 2070/10000, Loss: -1.9901\n",
            "Epoch 2080/10000, Loss: -1.9909\n",
            "Epoch 2090/10000, Loss: -1.9906\n",
            "Epoch 2100/10000, Loss: -1.9907\n",
            "Epoch 2110/10000, Loss: -1.9910\n",
            "Epoch 2120/10000, Loss: -1.9907\n",
            "Epoch 2130/10000, Loss: -1.9912\n",
            "Epoch 2140/10000, Loss: -1.9910\n",
            "Epoch 2150/10000, Loss: -1.9909\n",
            "Epoch 2160/10000, Loss: -1.9908\n",
            "Epoch 2170/10000, Loss: -1.9915\n",
            "Epoch 2180/10000, Loss: -1.9916\n",
            "Epoch 2190/10000, Loss: -1.9909\n",
            "Epoch 2200/10000, Loss: -1.9913\n",
            "Epoch 2210/10000, Loss: -1.9920\n",
            "Epoch 2220/10000, Loss: -1.9911\n",
            "Epoch 2230/10000, Loss: -1.9917\n",
            "Epoch 2240/10000, Loss: -1.9913\n",
            "Epoch 2250/10000, Loss: -1.9914\n",
            "Epoch 2260/10000, Loss: -1.9918\n",
            "Epoch 2270/10000, Loss: -1.9920\n",
            "Epoch 2280/10000, Loss: -1.9912\n",
            "Epoch 2290/10000, Loss: -1.9911\n",
            "Epoch 2300/10000, Loss: -1.9923\n",
            "Epoch 2310/10000, Loss: -1.9919\n",
            "Epoch 2320/10000, Loss: -1.9916\n",
            "Epoch 2330/10000, Loss: -1.9922\n",
            "Epoch 2340/10000, Loss: -1.9918\n",
            "Epoch 2350/10000, Loss: -1.9923\n",
            "Epoch 2360/10000, Loss: -1.9923\n",
            "Epoch 2370/10000, Loss: -1.9922\n",
            "Epoch 2380/10000, Loss: -1.9924\n",
            "Epoch 2390/10000, Loss: -1.9927\n",
            "Epoch 2400/10000, Loss: -1.9922\n",
            "Epoch 2410/10000, Loss: -1.9924\n",
            "Epoch 2420/10000, Loss: -1.9922\n",
            "Epoch 2430/10000, Loss: -1.9928\n",
            "Epoch 2440/10000, Loss: -1.9926\n",
            "Epoch 2450/10000, Loss: -1.9919\n",
            "Epoch 2460/10000, Loss: -1.9928\n",
            "Epoch 2470/10000, Loss: -1.9927\n",
            "Epoch 2480/10000, Loss: -1.9932\n",
            "Epoch 2490/10000, Loss: -1.9929\n",
            "Epoch 2500/10000, Loss: -1.9933\n",
            "Epoch 2510/10000, Loss: -1.9929\n",
            "Epoch 2520/10000, Loss: -1.9928\n",
            "Epoch 2530/10000, Loss: -1.9933\n",
            "Epoch 2540/10000, Loss: -1.9930\n",
            "Epoch 2550/10000, Loss: -1.9936\n",
            "Epoch 2560/10000, Loss: -1.9936\n",
            "Epoch 2570/10000, Loss: -1.9933\n",
            "Epoch 2580/10000, Loss: -1.9929\n",
            "Epoch 2590/10000, Loss: -1.9928\n",
            "Epoch 2600/10000, Loss: -1.9934\n",
            "Epoch 2610/10000, Loss: -1.9937\n",
            "Epoch 2620/10000, Loss: -1.9933\n",
            "Epoch 2630/10000, Loss: -1.9941\n",
            "Epoch 2640/10000, Loss: -1.9934\n",
            "Epoch 2650/10000, Loss: -1.9936\n",
            "Epoch 2660/10000, Loss: -1.9936\n",
            "Epoch 2670/10000, Loss: -1.9939\n",
            "Epoch 2680/10000, Loss: -1.9941\n",
            "Epoch 2690/10000, Loss: -1.9935\n",
            "Epoch 2700/10000, Loss: -1.9940\n",
            "Epoch 2710/10000, Loss: -1.9938\n",
            "Epoch 2720/10000, Loss: -1.9938\n",
            "Epoch 2730/10000, Loss: -1.9948\n",
            "Epoch 2740/10000, Loss: -1.9938\n",
            "Epoch 2750/10000, Loss: -1.9944\n",
            "Epoch 2760/10000, Loss: -1.9940\n",
            "Epoch 2770/10000, Loss: -1.9940\n",
            "Epoch 2780/10000, Loss: -1.9944\n",
            "Epoch 2790/10000, Loss: -1.9945\n",
            "Epoch 2800/10000, Loss: -1.9944\n",
            "Epoch 2810/10000, Loss: -1.9943\n",
            "Epoch 2820/10000, Loss: -1.9940\n",
            "Epoch 2830/10000, Loss: -1.9941\n",
            "Epoch 2840/10000, Loss: -1.9936\n",
            "Epoch 2850/10000, Loss: -1.9948\n",
            "Epoch 2860/10000, Loss: -1.9954\n",
            "Epoch 2870/10000, Loss: -1.9948\n",
            "Epoch 2880/10000, Loss: -1.9949\n",
            "Epoch 2890/10000, Loss: -1.9942\n",
            "Epoch 2900/10000, Loss: -1.9943\n",
            "Epoch 2910/10000, Loss: -1.9943\n",
            "Epoch 2920/10000, Loss: -1.9955\n",
            "Epoch 2930/10000, Loss: -1.9951\n",
            "Epoch 2940/10000, Loss: -1.9950\n",
            "Epoch 2950/10000, Loss: -1.9951\n",
            "Epoch 2960/10000, Loss: -1.9950\n",
            "Epoch 2970/10000, Loss: -1.9945\n",
            "Epoch 2980/10000, Loss: -1.9945\n",
            "Epoch 2990/10000, Loss: -1.9951\n",
            "Epoch 3000/10000, Loss: -1.9951\n",
            "Epoch 3010/10000, Loss: -1.9951\n",
            "Epoch 3020/10000, Loss: -1.9953\n",
            "Epoch 3030/10000, Loss: -1.9951\n",
            "Epoch 3040/10000, Loss: -1.9950\n",
            "Epoch 3050/10000, Loss: -1.9953\n",
            "Epoch 3060/10000, Loss: -1.9954\n",
            "Epoch 3070/10000, Loss: -1.9964\n",
            "Epoch 3080/10000, Loss: -1.9958\n",
            "Epoch 3090/10000, Loss: -1.9951\n",
            "Epoch 3100/10000, Loss: -1.9956\n",
            "Epoch 3110/10000, Loss: -1.9951\n",
            "Epoch 3120/10000, Loss: -1.9957\n",
            "Epoch 3130/10000, Loss: -1.9961\n",
            "Epoch 3140/10000, Loss: -1.9961\n",
            "Epoch 3150/10000, Loss: -1.9964\n",
            "Epoch 3160/10000, Loss: -1.9962\n",
            "Epoch 3170/10000, Loss: -1.9965\n",
            "Epoch 3180/10000, Loss: -1.9961\n",
            "Epoch 3190/10000, Loss: -1.9957\n",
            "Epoch 3200/10000, Loss: -1.9960\n",
            "Epoch 3210/10000, Loss: -1.9967\n",
            "Epoch 3220/10000, Loss: -1.9966\n",
            "Epoch 3230/10000, Loss: -1.9969\n",
            "Epoch 3240/10000, Loss: -1.9963\n",
            "Epoch 3250/10000, Loss: -1.9958\n",
            "Epoch 3260/10000, Loss: -1.9969\n",
            "Epoch 3270/10000, Loss: -1.9966\n",
            "Epoch 3280/10000, Loss: -1.9968\n",
            "Epoch 3290/10000, Loss: -1.9967\n",
            "Epoch 3300/10000, Loss: -1.9977\n",
            "Epoch 3310/10000, Loss: -1.9964\n",
            "Epoch 3320/10000, Loss: -1.9965\n",
            "Epoch 3330/10000, Loss: -1.9967\n",
            "Epoch 3340/10000, Loss: -1.9969\n",
            "Epoch 3350/10000, Loss: -1.9975\n",
            "Epoch 3360/10000, Loss: -1.9970\n",
            "Epoch 3370/10000, Loss: -1.9969\n",
            "Epoch 3380/10000, Loss: -1.9974\n",
            "Epoch 3390/10000, Loss: -1.9974\n",
            "Epoch 3400/10000, Loss: -1.9975\n",
            "Epoch 3410/10000, Loss: -1.9978\n",
            "Epoch 3420/10000, Loss: -1.9976\n",
            "Epoch 3430/10000, Loss: -1.9977\n",
            "Epoch 3440/10000, Loss: -1.9970\n",
            "Epoch 3450/10000, Loss: -1.9979\n",
            "Epoch 3460/10000, Loss: -1.9962\n",
            "Epoch 3470/10000, Loss: -1.9971\n",
            "Epoch 3480/10000, Loss: -1.9985\n",
            "Epoch 3490/10000, Loss: -1.9976\n",
            "Epoch 3500/10000, Loss: -1.9976\n",
            "Epoch 3510/10000, Loss: -1.9976\n",
            "Epoch 3520/10000, Loss: -1.9978\n",
            "Epoch 3530/10000, Loss: -1.9987\n",
            "Epoch 3540/10000, Loss: -1.9975\n",
            "Epoch 3550/10000, Loss: -1.9983\n",
            "Epoch 3560/10000, Loss: -1.9978\n",
            "Epoch 3570/10000, Loss: -1.9980\n",
            "Epoch 3580/10000, Loss: -1.9986\n",
            "Epoch 3590/10000, Loss: -1.9978\n",
            "Epoch 3600/10000, Loss: -1.9984\n",
            "Epoch 3610/10000, Loss: -1.9988\n",
            "Epoch 3620/10000, Loss: -1.9986\n",
            "Epoch 3630/10000, Loss: -1.9983\n",
            "Epoch 3640/10000, Loss: -1.9989\n",
            "Epoch 3650/10000, Loss: -1.9987\n",
            "Epoch 3660/10000, Loss: -1.9984\n",
            "Epoch 3670/10000, Loss: -1.9986\n",
            "Epoch 3680/10000, Loss: -1.9992\n",
            "Epoch 3690/10000, Loss: -1.9984\n",
            "Epoch 3700/10000, Loss: -1.9989\n",
            "Epoch 3710/10000, Loss: -1.9992\n",
            "Epoch 3720/10000, Loss: -1.9988\n",
            "Epoch 3730/10000, Loss: -1.9987\n",
            "Epoch 3740/10000, Loss: -1.9990\n",
            "Epoch 3750/10000, Loss: -1.9992\n",
            "Epoch 3760/10000, Loss: -1.9993\n",
            "Epoch 3770/10000, Loss: -1.9988\n",
            "Epoch 3780/10000, Loss: -1.9990\n",
            "Epoch 3790/10000, Loss: -1.9988\n",
            "Epoch 3800/10000, Loss: -1.9997\n",
            "Epoch 3810/10000, Loss: -1.9996\n",
            "Epoch 3820/10000, Loss: -1.9990\n",
            "Epoch 3830/10000, Loss: -1.9987\n",
            "Epoch 3840/10000, Loss: -1.9997\n",
            "Epoch 3850/10000, Loss: -1.9996\n",
            "Epoch 3860/10000, Loss: -1.9998\n",
            "Epoch 3870/10000, Loss: -1.9995\n",
            "Epoch 3880/10000, Loss: -1.9996\n",
            "Epoch 3890/10000, Loss: -1.9992\n",
            "Epoch 3900/10000, Loss: -1.9992\n",
            "Epoch 3910/10000, Loss: -1.9999\n",
            "Epoch 3920/10000, Loss: -1.9995\n",
            "Epoch 3930/10000, Loss: -1.9999\n",
            "Epoch 3940/10000, Loss: -1.9992\n",
            "Epoch 3950/10000, Loss: -2.0001\n",
            "Epoch 3960/10000, Loss: -1.9994\n",
            "Epoch 3970/10000, Loss: -2.0001\n",
            "Epoch 3980/10000, Loss: -1.9997\n",
            "Epoch 3990/10000, Loss: -1.9996\n",
            "Epoch 4000/10000, Loss: -2.0000\n",
            "Epoch 4010/10000, Loss: -1.9991\n",
            "Epoch 4020/10000, Loss: -2.0008\n",
            "Epoch 4030/10000, Loss: -2.0000\n",
            "Epoch 4040/10000, Loss: -2.0006\n",
            "Epoch 4050/10000, Loss: -2.0001\n",
            "Epoch 4060/10000, Loss: -2.0005\n",
            "Epoch 4070/10000, Loss: -1.9994\n",
            "Epoch 4080/10000, Loss: -2.0001\n",
            "Epoch 4090/10000, Loss: -2.0004\n",
            "Epoch 4100/10000, Loss: -1.9998\n",
            "Epoch 4110/10000, Loss: -2.0002\n",
            "Epoch 4120/10000, Loss: -2.0002\n",
            "Epoch 4130/10000, Loss: -2.0001\n",
            "Epoch 4140/10000, Loss: -2.0004\n",
            "Epoch 4150/10000, Loss: -2.0008\n",
            "Epoch 4160/10000, Loss: -1.9996\n",
            "Epoch 4170/10000, Loss: -2.0000\n",
            "Epoch 4180/10000, Loss: -2.0014\n",
            "Epoch 4190/10000, Loss: -2.0006\n",
            "Epoch 4200/10000, Loss: -2.0007\n",
            "Epoch 4210/10000, Loss: -2.0011\n",
            "Epoch 4220/10000, Loss: -2.0009\n",
            "Epoch 4230/10000, Loss: -2.0005\n",
            "Epoch 4240/10000, Loss: -2.0003\n",
            "Epoch 4250/10000, Loss: -2.0011\n",
            "Epoch 4260/10000, Loss: -2.0013\n",
            "Epoch 4270/10000, Loss: -2.0010\n",
            "Epoch 4280/10000, Loss: -2.0013\n",
            "Epoch 4290/10000, Loss: -2.0010\n",
            "Epoch 4300/10000, Loss: -2.0017\n",
            "Epoch 4310/10000, Loss: -2.0013\n",
            "Epoch 4320/10000, Loss: -2.0010\n",
            "Epoch 4330/10000, Loss: -2.0016\n",
            "Epoch 4340/10000, Loss: -2.0012\n",
            "Epoch 4350/10000, Loss: -2.0013\n",
            "Epoch 4360/10000, Loss: -2.0008\n",
            "Epoch 4370/10000, Loss: -2.0014\n",
            "Epoch 4380/10000, Loss: -2.0016\n",
            "Epoch 4390/10000, Loss: -2.0015\n",
            "Epoch 4400/10000, Loss: -2.0015\n",
            "Epoch 4410/10000, Loss: -2.0008\n",
            "Epoch 4420/10000, Loss: -2.0016\n",
            "Epoch 4430/10000, Loss: -2.0019\n",
            "Epoch 4440/10000, Loss: -2.0016\n",
            "Epoch 4450/10000, Loss: -2.0016\n",
            "Epoch 4460/10000, Loss: -2.0020\n",
            "Epoch 4470/10000, Loss: -2.0019\n",
            "Epoch 4480/10000, Loss: -2.0015\n",
            "Epoch 4490/10000, Loss: -2.0018\n",
            "Epoch 4500/10000, Loss: -2.0014\n",
            "Epoch 4510/10000, Loss: -2.0019\n",
            "Epoch 4520/10000, Loss: -2.0017\n",
            "Epoch 4530/10000, Loss: -2.0017\n",
            "Epoch 4540/10000, Loss: -2.0022\n",
            "Epoch 4550/10000, Loss: -2.0017\n",
            "Epoch 4560/10000, Loss: -2.0025\n",
            "Epoch 4570/10000, Loss: -2.0020\n",
            "Epoch 4580/10000, Loss: -2.0015\n",
            "Epoch 4590/10000, Loss: -2.0021\n",
            "Epoch 4600/10000, Loss: -2.0023\n",
            "Epoch 4610/10000, Loss: -2.0021\n",
            "Epoch 4620/10000, Loss: -2.0017\n",
            "Epoch 4630/10000, Loss: -2.0021\n",
            "Epoch 4640/10000, Loss: -2.0028\n",
            "Epoch 4650/10000, Loss: -2.0029\n",
            "Epoch 4660/10000, Loss: -2.0021\n",
            "Epoch 4670/10000, Loss: -2.0022\n",
            "Epoch 4680/10000, Loss: -2.0025\n",
            "Epoch 4690/10000, Loss: -2.0026\n",
            "Epoch 4700/10000, Loss: -2.0025\n",
            "Epoch 4710/10000, Loss: -2.0029\n",
            "Epoch 4720/10000, Loss: -2.0027\n",
            "Epoch 4730/10000, Loss: -2.0019\n",
            "Epoch 4740/10000, Loss: -2.0022\n",
            "Epoch 4750/10000, Loss: -2.0021\n",
            "Epoch 4760/10000, Loss: -2.0020\n",
            "Epoch 4770/10000, Loss: -2.0022\n",
            "Epoch 4780/10000, Loss: -2.0025\n",
            "Epoch 4790/10000, Loss: -2.0022\n",
            "Epoch 4800/10000, Loss: -2.0026\n",
            "Epoch 4810/10000, Loss: -2.0025\n",
            "Epoch 4820/10000, Loss: -2.0034\n",
            "Epoch 4830/10000, Loss: -2.0029\n",
            "Epoch 4840/10000, Loss: -2.0030\n",
            "Epoch 4850/10000, Loss: -2.0031\n",
            "Epoch 4860/10000, Loss: -2.0031\n",
            "Epoch 4870/10000, Loss: -2.0030\n",
            "Epoch 4880/10000, Loss: -2.0036\n",
            "Epoch 4890/10000, Loss: -2.0027\n",
            "Epoch 4900/10000, Loss: -2.0033\n",
            "Epoch 4910/10000, Loss: -2.0025\n",
            "Epoch 4920/10000, Loss: -2.0034\n",
            "Epoch 4930/10000, Loss: -2.0031\n",
            "Epoch 4940/10000, Loss: -2.0035\n",
            "Epoch 4950/10000, Loss: -2.0035\n",
            "Epoch 4960/10000, Loss: -2.0032\n",
            "Epoch 4970/10000, Loss: -2.0030\n",
            "Epoch 4980/10000, Loss: -2.0028\n",
            "Epoch 4990/10000, Loss: -2.0036\n",
            "Epoch 5000/10000, Loss: -2.0031\n",
            "Epoch 5010/10000, Loss: -2.0029\n",
            "Epoch 5020/10000, Loss: -2.0031\n",
            "Epoch 5030/10000, Loss: -2.0030\n",
            "Epoch 5040/10000, Loss: -2.0031\n",
            "Epoch 5050/10000, Loss: -2.0029\n",
            "Epoch 5060/10000, Loss: -2.0030\n",
            "Epoch 5070/10000, Loss: -2.0035\n",
            "Epoch 5080/10000, Loss: -2.0042\n",
            "Epoch 5090/10000, Loss: -2.0033\n",
            "Epoch 5100/10000, Loss: -2.0026\n",
            "Epoch 5110/10000, Loss: -2.0040\n",
            "Epoch 5120/10000, Loss: -2.0032\n",
            "Epoch 5130/10000, Loss: -2.0039\n",
            "Epoch 5140/10000, Loss: -2.0031\n",
            "Epoch 5150/10000, Loss: -2.0030\n",
            "Epoch 5160/10000, Loss: -2.0041\n",
            "Epoch 5170/10000, Loss: -2.0039\n",
            "Epoch 5180/10000, Loss: -2.0040\n",
            "Epoch 5190/10000, Loss: -2.0036\n",
            "Epoch 5200/10000, Loss: -2.0037\n",
            "Epoch 5210/10000, Loss: -2.0034\n",
            "Epoch 5220/10000, Loss: -2.0039\n",
            "Epoch 5230/10000, Loss: -2.0039\n",
            "Epoch 5240/10000, Loss: -2.0039\n",
            "Epoch 5250/10000, Loss: -2.0043\n",
            "Epoch 5260/10000, Loss: -2.0035\n",
            "Epoch 5270/10000, Loss: -2.0046\n",
            "Epoch 5280/10000, Loss: -2.0031\n",
            "Epoch 5290/10000, Loss: -2.0035\n",
            "Epoch 5300/10000, Loss: -2.0043\n",
            "Epoch 5310/10000, Loss: -2.0047\n",
            "Epoch 5320/10000, Loss: -2.0043\n",
            "Epoch 5330/10000, Loss: -2.0042\n",
            "Epoch 5340/10000, Loss: -2.0048\n",
            "Epoch 5350/10000, Loss: -2.0049\n",
            "Epoch 5360/10000, Loss: -2.0039\n",
            "Epoch 5370/10000, Loss: -2.0046\n",
            "Epoch 5380/10000, Loss: -2.0045\n",
            "Epoch 5390/10000, Loss: -2.0032\n",
            "Epoch 5400/10000, Loss: -2.0044\n",
            "Epoch 5410/10000, Loss: -2.0044\n",
            "Epoch 5420/10000, Loss: -2.0043\n",
            "Epoch 5430/10000, Loss: -2.0046\n",
            "Epoch 5440/10000, Loss: -2.0050\n",
            "Epoch 5450/10000, Loss: -2.0049\n",
            "Epoch 5460/10000, Loss: -2.0046\n",
            "Epoch 5470/10000, Loss: -2.0044\n",
            "Epoch 5480/10000, Loss: -2.0041\n",
            "Epoch 5490/10000, Loss: -2.0054\n",
            "Epoch 5500/10000, Loss: -2.0043\n",
            "Epoch 5510/10000, Loss: -2.0048\n",
            "Epoch 5520/10000, Loss: -2.0051\n",
            "Epoch 5530/10000, Loss: -2.0046\n",
            "Epoch 5540/10000, Loss: -2.0035\n",
            "Epoch 5550/10000, Loss: -2.0052\n",
            "Epoch 5560/10000, Loss: -2.0048\n",
            "Epoch 5570/10000, Loss: -2.0048\n",
            "Epoch 5580/10000, Loss: -2.0049\n",
            "Epoch 5590/10000, Loss: -2.0047\n",
            "Epoch 5600/10000, Loss: -2.0043\n",
            "Epoch 5610/10000, Loss: -2.0045\n",
            "Epoch 5620/10000, Loss: -2.0055\n",
            "Epoch 5630/10000, Loss: -2.0046\n",
            "Epoch 5640/10000, Loss: -2.0049\n",
            "Epoch 5650/10000, Loss: -2.0048\n",
            "Epoch 5660/10000, Loss: -2.0048\n",
            "Epoch 5670/10000, Loss: -2.0049\n",
            "Epoch 5680/10000, Loss: -2.0049\n",
            "Epoch 5690/10000, Loss: -2.0056\n",
            "Epoch 5700/10000, Loss: -2.0048\n",
            "Epoch 5710/10000, Loss: -2.0045\n",
            "Epoch 5720/10000, Loss: -2.0052\n",
            "Epoch 5730/10000, Loss: -2.0048\n",
            "Epoch 5740/10000, Loss: -2.0053\n",
            "Epoch 5750/10000, Loss: -2.0056\n",
            "Epoch 5760/10000, Loss: -2.0049\n",
            "Epoch 5770/10000, Loss: -2.0055\n",
            "Epoch 5780/10000, Loss: -2.0045\n",
            "Epoch 5790/10000, Loss: -2.0051\n",
            "Epoch 5800/10000, Loss: -2.0047\n",
            "Epoch 5810/10000, Loss: -2.0054\n",
            "Epoch 5820/10000, Loss: -2.0050\n",
            "Epoch 5830/10000, Loss: -2.0053\n",
            "Epoch 5840/10000, Loss: -2.0054\n",
            "Epoch 5850/10000, Loss: -2.0052\n",
            "Epoch 5860/10000, Loss: -2.0049\n",
            "Epoch 5870/10000, Loss: -2.0049\n",
            "Epoch 5880/10000, Loss: -2.0045\n",
            "Epoch 5890/10000, Loss: -2.0059\n",
            "Epoch 5900/10000, Loss: -2.0057\n",
            "Epoch 5910/10000, Loss: -2.0038\n",
            "Epoch 5920/10000, Loss: -2.0052\n",
            "Epoch 5930/10000, Loss: -2.0052\n",
            "Epoch 5940/10000, Loss: -2.0052\n",
            "Epoch 5950/10000, Loss: -2.0050\n",
            "Epoch 5960/10000, Loss: -2.0052\n",
            "Epoch 5970/10000, Loss: -2.0052\n",
            "Epoch 5980/10000, Loss: -2.0059\n",
            "Epoch 5990/10000, Loss: -2.0058\n",
            "Epoch 6000/10000, Loss: -2.0053\n",
            "Epoch 6010/10000, Loss: -2.0052\n",
            "Epoch 6020/10000, Loss: -2.0049\n",
            "Epoch 6030/10000, Loss: -2.0053\n",
            "Epoch 6040/10000, Loss: -2.0057\n",
            "Epoch 6050/10000, Loss: -2.0055\n",
            "Epoch 6060/10000, Loss: -2.0058\n",
            "Epoch 6070/10000, Loss: -2.0058\n",
            "Epoch 6080/10000, Loss: -2.0059\n",
            "Epoch 6090/10000, Loss: -2.0059\n",
            "Epoch 6100/10000, Loss: -2.0054\n",
            "Epoch 6110/10000, Loss: -2.0065\n",
            "Epoch 6120/10000, Loss: -2.0057\n",
            "Epoch 6130/10000, Loss: -2.0057\n",
            "Epoch 6140/10000, Loss: -2.0053\n",
            "Epoch 6150/10000, Loss: -2.0053\n",
            "Epoch 6160/10000, Loss: -2.0058\n",
            "Epoch 6170/10000, Loss: -2.0057\n",
            "Epoch 6180/10000, Loss: -2.0060\n",
            "Epoch 6190/10000, Loss: -2.0066\n",
            "Epoch 6200/10000, Loss: -2.0060\n",
            "Epoch 6210/10000, Loss: -2.0059\n",
            "Epoch 6220/10000, Loss: -2.0055\n",
            "Epoch 6230/10000, Loss: -2.0065\n",
            "Epoch 6240/10000, Loss: -2.0058\n",
            "Epoch 6250/10000, Loss: -2.0058\n",
            "Epoch 6260/10000, Loss: -2.0059\n",
            "Epoch 6270/10000, Loss: -2.0056\n",
            "Epoch 6280/10000, Loss: -2.0057\n",
            "Epoch 6290/10000, Loss: -2.0062\n",
            "Epoch 6300/10000, Loss: -2.0057\n",
            "Epoch 6310/10000, Loss: -2.0056\n",
            "Epoch 6320/10000, Loss: -2.0062\n",
            "Epoch 6330/10000, Loss: -2.0062\n",
            "Epoch 6340/10000, Loss: -2.0064\n",
            "Epoch 6350/10000, Loss: -2.0056\n",
            "Epoch 6360/10000, Loss: -2.0059\n",
            "Epoch 6370/10000, Loss: -2.0059\n",
            "Epoch 6380/10000, Loss: -2.0064\n",
            "Epoch 6390/10000, Loss: -2.0062\n",
            "Epoch 6400/10000, Loss: -2.0052\n",
            "Epoch 6410/10000, Loss: -2.0057\n",
            "Epoch 6420/10000, Loss: -2.0059\n",
            "Epoch 6430/10000, Loss: -2.0062\n",
            "Epoch 6440/10000, Loss: -2.0064\n",
            "Epoch 6450/10000, Loss: -2.0065\n",
            "Epoch 6460/10000, Loss: -2.0063\n",
            "Epoch 6470/10000, Loss: -2.0062\n",
            "Epoch 6480/10000, Loss: -2.0062\n",
            "Epoch 6490/10000, Loss: -2.0063\n",
            "Epoch 6500/10000, Loss: -2.0059\n",
            "Epoch 6510/10000, Loss: -2.0057\n",
            "Epoch 6520/10000, Loss: -2.0064\n",
            "Epoch 6530/10000, Loss: -2.0064\n",
            "Epoch 6540/10000, Loss: -2.0062\n",
            "Epoch 6550/10000, Loss: -2.0064\n",
            "Epoch 6560/10000, Loss: -2.0054\n",
            "Epoch 6570/10000, Loss: -2.0060\n",
            "Epoch 6580/10000, Loss: -2.0059\n",
            "Epoch 6590/10000, Loss: -2.0061\n",
            "Epoch 6600/10000, Loss: -2.0064\n",
            "Epoch 6610/10000, Loss: -2.0057\n",
            "Epoch 6620/10000, Loss: -2.0062\n",
            "Epoch 6630/10000, Loss: -2.0066\n",
            "Epoch 6640/10000, Loss: -2.0055\n",
            "Epoch 6650/10000, Loss: -2.0064\n",
            "Epoch 6660/10000, Loss: -2.0068\n",
            "Epoch 6670/10000, Loss: -2.0065\n",
            "Epoch 6680/10000, Loss: -2.0065\n",
            "Epoch 6690/10000, Loss: -2.0065\n",
            "Epoch 6700/10000, Loss: -2.0069\n",
            "Epoch 6710/10000, Loss: -2.0064\n",
            "Epoch 6720/10000, Loss: -2.0069\n",
            "Epoch 6730/10000, Loss: -2.0061\n",
            "Epoch 6740/10000, Loss: -2.0063\n",
            "Epoch 6750/10000, Loss: -2.0058\n",
            "Epoch 6760/10000, Loss: -2.0062\n",
            "Epoch 6770/10000, Loss: -2.0062\n",
            "Epoch 6780/10000, Loss: -2.0074\n",
            "Epoch 6790/10000, Loss: -2.0065\n",
            "Epoch 6800/10000, Loss: -2.0068\n",
            "Epoch 6810/10000, Loss: -2.0067\n",
            "Epoch 6820/10000, Loss: -2.0067\n",
            "Epoch 6830/10000, Loss: -2.0073\n",
            "Epoch 6840/10000, Loss: -2.0070\n",
            "Epoch 6850/10000, Loss: -2.0067\n",
            "Epoch 6860/10000, Loss: -2.0067\n",
            "Epoch 6870/10000, Loss: -2.0067\n",
            "Epoch 6880/10000, Loss: -2.0068\n",
            "Epoch 6890/10000, Loss: -2.0067\n",
            "Epoch 6900/10000, Loss: -2.0067\n",
            "Epoch 6910/10000, Loss: -2.0068\n",
            "Epoch 6920/10000, Loss: -2.0070\n",
            "Epoch 6930/10000, Loss: -2.0068\n",
            "Epoch 6940/10000, Loss: -2.0059\n",
            "Epoch 6950/10000, Loss: -2.0063\n",
            "Epoch 6960/10000, Loss: -2.0067\n",
            "Epoch 6970/10000, Loss: -2.0072\n",
            "Epoch 6980/10000, Loss: -2.0063\n",
            "Epoch 6990/10000, Loss: -2.0067\n",
            "Epoch 7000/10000, Loss: -2.0068\n",
            "Epoch 7010/10000, Loss: -2.0068\n",
            "Epoch 7020/10000, Loss: -2.0068\n",
            "Epoch 7030/10000, Loss: -2.0061\n",
            "Epoch 7040/10000, Loss: -2.0064\n",
            "Epoch 7050/10000, Loss: -2.0063\n",
            "Epoch 7060/10000, Loss: -2.0071\n",
            "Epoch 7070/10000, Loss: -2.0066\n",
            "Epoch 7080/10000, Loss: -2.0064\n",
            "Epoch 7090/10000, Loss: -2.0066\n",
            "Epoch 7100/10000, Loss: -2.0062\n",
            "Epoch 7110/10000, Loss: -2.0073\n",
            "Epoch 7120/10000, Loss: -2.0075\n",
            "Epoch 7130/10000, Loss: -2.0071\n",
            "Epoch 7140/10000, Loss: -2.0071\n",
            "Epoch 7150/10000, Loss: -2.0072\n",
            "Epoch 7160/10000, Loss: -2.0080\n",
            "Epoch 7170/10000, Loss: -2.0070\n",
            "Epoch 7180/10000, Loss: -2.0075\n",
            "Epoch 7190/10000, Loss: -2.0073\n",
            "Epoch 7200/10000, Loss: -2.0067\n",
            "Epoch 7210/10000, Loss: -2.0066\n",
            "Epoch 7220/10000, Loss: -2.0074\n",
            "Epoch 7230/10000, Loss: -2.0073\n",
            "Epoch 7240/10000, Loss: -2.0073\n",
            "Epoch 7250/10000, Loss: -2.0064\n",
            "Epoch 7260/10000, Loss: -2.0070\n",
            "Epoch 7270/10000, Loss: -2.0073\n",
            "Epoch 7280/10000, Loss: -2.0072\n",
            "Epoch 7290/10000, Loss: -2.0072\n",
            "Epoch 7300/10000, Loss: -2.0068\n",
            "Epoch 7310/10000, Loss: -2.0070\n",
            "Epoch 7320/10000, Loss: -2.0073\n",
            "Epoch 7330/10000, Loss: -2.0072\n",
            "Epoch 7340/10000, Loss: -2.0076\n",
            "Epoch 7350/10000, Loss: -2.0077\n",
            "Epoch 7360/10000, Loss: -2.0069\n",
            "Epoch 7370/10000, Loss: -2.0071\n",
            "Epoch 7380/10000, Loss: -2.0070\n",
            "Epoch 7390/10000, Loss: -2.0072\n",
            "Epoch 7400/10000, Loss: -2.0074\n",
            "Epoch 7410/10000, Loss: -2.0073\n",
            "Epoch 7420/10000, Loss: -2.0074\n",
            "Epoch 7430/10000, Loss: -2.0073\n",
            "Epoch 7440/10000, Loss: -2.0075\n",
            "Epoch 7450/10000, Loss: -2.0070\n",
            "Epoch 7460/10000, Loss: -2.0075\n",
            "Epoch 7470/10000, Loss: -2.0073\n",
            "Epoch 7480/10000, Loss: -2.0083\n",
            "Epoch 7490/10000, Loss: -2.0073\n",
            "Epoch 7500/10000, Loss: -2.0072\n",
            "Epoch 7510/10000, Loss: -2.0074\n",
            "Epoch 7520/10000, Loss: -2.0077\n",
            "Epoch 7530/10000, Loss: -2.0071\n",
            "Epoch 7540/10000, Loss: -2.0076\n",
            "Epoch 7550/10000, Loss: -2.0073\n",
            "Epoch 7560/10000, Loss: -2.0067\n",
            "Epoch 7570/10000, Loss: -2.0080\n",
            "Epoch 7580/10000, Loss: -2.0073\n",
            "Epoch 7590/10000, Loss: -2.0071\n",
            "Epoch 7600/10000, Loss: -2.0068\n",
            "Epoch 7610/10000, Loss: -2.0073\n",
            "Epoch 7620/10000, Loss: -2.0073\n",
            "Epoch 7630/10000, Loss: -2.0078\n",
            "Epoch 7640/10000, Loss: -2.0077\n",
            "Epoch 7650/10000, Loss: -2.0080\n",
            "Epoch 7660/10000, Loss: -2.0072\n",
            "Epoch 7670/10000, Loss: -2.0077\n",
            "Epoch 7680/10000, Loss: -2.0076\n",
            "Epoch 7690/10000, Loss: -2.0075\n",
            "Epoch 7700/10000, Loss: -2.0077\n",
            "Epoch 7710/10000, Loss: -2.0074\n",
            "Epoch 7720/10000, Loss: -2.0075\n",
            "Epoch 7730/10000, Loss: -2.0081\n",
            "Epoch 7740/10000, Loss: -2.0082\n",
            "Epoch 7750/10000, Loss: -2.0087\n",
            "Epoch 7760/10000, Loss: -2.0070\n",
            "Epoch 7770/10000, Loss: -2.0073\n",
            "Epoch 7780/10000, Loss: -2.0076\n",
            "Epoch 7790/10000, Loss: -2.0079\n",
            "Epoch 7800/10000, Loss: -2.0075\n",
            "Epoch 7810/10000, Loss: -2.0069\n",
            "Epoch 7820/10000, Loss: -2.0074\n",
            "Epoch 7830/10000, Loss: -2.0071\n",
            "Epoch 7840/10000, Loss: -2.0082\n",
            "Epoch 7850/10000, Loss: -2.0070\n",
            "Epoch 7860/10000, Loss: -2.0076\n",
            "Epoch 7870/10000, Loss: -2.0081\n",
            "Epoch 7880/10000, Loss: -2.0080\n",
            "Epoch 7890/10000, Loss: -2.0074\n",
            "Epoch 7900/10000, Loss: -2.0075\n",
            "Epoch 7910/10000, Loss: -2.0081\n",
            "Epoch 7920/10000, Loss: -2.0079\n",
            "Epoch 7930/10000, Loss: -2.0081\n",
            "Epoch 7940/10000, Loss: -2.0076\n",
            "Epoch 7950/10000, Loss: -2.0078\n",
            "Epoch 7960/10000, Loss: -2.0074\n",
            "Epoch 7970/10000, Loss: -2.0081\n",
            "Epoch 7980/10000, Loss: -2.0077\n",
            "Epoch 7990/10000, Loss: -2.0085\n",
            "Epoch 8000/10000, Loss: -2.0077\n",
            "Epoch 8010/10000, Loss: -2.0080\n",
            "Epoch 8020/10000, Loss: -2.0079\n",
            "Epoch 8030/10000, Loss: -2.0081\n",
            "Epoch 8040/10000, Loss: -2.0078\n",
            "Epoch 8050/10000, Loss: -2.0082\n",
            "Epoch 8060/10000, Loss: -2.0084\n",
            "Epoch 8070/10000, Loss: -2.0079\n",
            "Epoch 8080/10000, Loss: -2.0076\n",
            "Epoch 8090/10000, Loss: -2.0085\n",
            "Epoch 8100/10000, Loss: -2.0080\n",
            "Epoch 8110/10000, Loss: -2.0075\n",
            "Epoch 8120/10000, Loss: -2.0082\n",
            "Epoch 8130/10000, Loss: -2.0090\n",
            "Epoch 8140/10000, Loss: -2.0081\n",
            "Epoch 8150/10000, Loss: -2.0077\n",
            "Epoch 8160/10000, Loss: -2.0082\n",
            "Epoch 8170/10000, Loss: -2.0081\n",
            "Epoch 8180/10000, Loss: -2.0087\n",
            "Epoch 8190/10000, Loss: -2.0079\n",
            "Epoch 8200/10000, Loss: -2.0083\n",
            "Epoch 8210/10000, Loss: -2.0080\n",
            "Epoch 8220/10000, Loss: -2.0081\n",
            "Epoch 8230/10000, Loss: -2.0080\n",
            "Epoch 8240/10000, Loss: -2.0083\n",
            "Epoch 8250/10000, Loss: -2.0084\n",
            "Epoch 8260/10000, Loss: -2.0073\n",
            "Epoch 8270/10000, Loss: -2.0087\n",
            "Epoch 8280/10000, Loss: -2.0083\n",
            "Epoch 8290/10000, Loss: -2.0085\n",
            "Epoch 8300/10000, Loss: -2.0069\n",
            "Epoch 8310/10000, Loss: -2.0081\n",
            "Epoch 8320/10000, Loss: -2.0084\n",
            "Epoch 8330/10000, Loss: -2.0089\n",
            "Epoch 8340/10000, Loss: -2.0082\n",
            "Epoch 8350/10000, Loss: -2.0086\n",
            "Epoch 8360/10000, Loss: -2.0088\n",
            "Epoch 8370/10000, Loss: -2.0088\n",
            "Epoch 8380/10000, Loss: -2.0073\n",
            "Epoch 8390/10000, Loss: -2.0089\n",
            "Epoch 8400/10000, Loss: -2.0082\n",
            "Epoch 8410/10000, Loss: -2.0086\n",
            "Epoch 8420/10000, Loss: -2.0092\n",
            "Epoch 8430/10000, Loss: -2.0083\n",
            "Epoch 8440/10000, Loss: -2.0085\n",
            "Epoch 8450/10000, Loss: -2.0085\n",
            "Epoch 8460/10000, Loss: -2.0087\n",
            "Epoch 8470/10000, Loss: -2.0087\n",
            "Epoch 8480/10000, Loss: -2.0082\n",
            "Epoch 8490/10000, Loss: -2.0089\n",
            "Epoch 8500/10000, Loss: -2.0086\n",
            "Epoch 8510/10000, Loss: -2.0086\n",
            "Epoch 8520/10000, Loss: -2.0094\n",
            "Epoch 8530/10000, Loss: -2.0092\n",
            "Epoch 8540/10000, Loss: -2.0080\n",
            "Epoch 8550/10000, Loss: -2.0087\n",
            "Epoch 8560/10000, Loss: -2.0085\n",
            "Epoch 8570/10000, Loss: -2.0088\n",
            "Epoch 8580/10000, Loss: -2.0088\n",
            "Epoch 8590/10000, Loss: -2.0089\n",
            "Epoch 8600/10000, Loss: -2.0089\n",
            "Epoch 8610/10000, Loss: -2.0086\n",
            "Epoch 8620/10000, Loss: -2.0088\n",
            "Epoch 8630/10000, Loss: -2.0085\n",
            "Epoch 8640/10000, Loss: -2.0087\n",
            "Epoch 8650/10000, Loss: -2.0088\n",
            "Epoch 8660/10000, Loss: -2.0080\n",
            "Epoch 8670/10000, Loss: -2.0085\n",
            "Epoch 8680/10000, Loss: -2.0089\n",
            "Epoch 8690/10000, Loss: -2.0080\n",
            "Epoch 8700/10000, Loss: -2.0090\n",
            "Epoch 8710/10000, Loss: -2.0083\n",
            "Epoch 8720/10000, Loss: -2.0093\n",
            "Epoch 8730/10000, Loss: -2.0087\n",
            "Epoch 8740/10000, Loss: -2.0083\n",
            "Epoch 8750/10000, Loss: -2.0087\n",
            "Epoch 8760/10000, Loss: -2.0086\n",
            "Epoch 8770/10000, Loss: -2.0084\n",
            "Epoch 8780/10000, Loss: -2.0090\n",
            "Epoch 8790/10000, Loss: -2.0085\n",
            "Epoch 8800/10000, Loss: -2.0087\n",
            "Epoch 8810/10000, Loss: -2.0098\n",
            "Epoch 8820/10000, Loss: -2.0090\n",
            "Epoch 8830/10000, Loss: -2.0085\n",
            "Epoch 8840/10000, Loss: -2.0086\n",
            "Epoch 8850/10000, Loss: -2.0090\n",
            "Epoch 8860/10000, Loss: -2.0092\n",
            "Epoch 8870/10000, Loss: -2.0084\n",
            "Epoch 8880/10000, Loss: -2.0081\n",
            "Epoch 8890/10000, Loss: -2.0089\n",
            "Epoch 8900/10000, Loss: -2.0078\n",
            "Epoch 8910/10000, Loss: -2.0088\n",
            "Epoch 8920/10000, Loss: -2.0091\n",
            "Epoch 8930/10000, Loss: -2.0091\n",
            "Epoch 8940/10000, Loss: -2.0093\n",
            "Epoch 8950/10000, Loss: -2.0076\n",
            "Epoch 8960/10000, Loss: -2.0086\n",
            "Epoch 8970/10000, Loss: -2.0090\n",
            "Epoch 8980/10000, Loss: -2.0088\n",
            "Epoch 8990/10000, Loss: -2.0086\n",
            "Epoch 9000/10000, Loss: -2.0093\n",
            "Epoch 9010/10000, Loss: -2.0091\n",
            "Epoch 9020/10000, Loss: -2.0091\n",
            "Epoch 9030/10000, Loss: -2.0090\n",
            "Epoch 9040/10000, Loss: -2.0090\n",
            "Epoch 9050/10000, Loss: -2.0092\n",
            "Epoch 9060/10000, Loss: -2.0091\n",
            "Epoch 9070/10000, Loss: -2.0087\n",
            "Epoch 9080/10000, Loss: -2.0095\n",
            "Epoch 9090/10000, Loss: -2.0096\n",
            "Epoch 9100/10000, Loss: -2.0096\n",
            "Epoch 9110/10000, Loss: -2.0090\n",
            "Epoch 9120/10000, Loss: -2.0088\n",
            "Epoch 9130/10000, Loss: -2.0091\n",
            "Epoch 9140/10000, Loss: -2.0089\n",
            "Epoch 9150/10000, Loss: -2.0086\n",
            "Epoch 9160/10000, Loss: -2.0089\n",
            "Epoch 9170/10000, Loss: -2.0090\n",
            "Epoch 9180/10000, Loss: -2.0090\n",
            "Epoch 9190/10000, Loss: -2.0094\n",
            "Epoch 9200/10000, Loss: -2.0089\n",
            "Epoch 9210/10000, Loss: -2.0094\n",
            "Epoch 9220/10000, Loss: -2.0095\n",
            "Epoch 9230/10000, Loss: -2.0091\n",
            "Epoch 9240/10000, Loss: -2.0090\n",
            "Epoch 9250/10000, Loss: -2.0090\n",
            "Epoch 9260/10000, Loss: -2.0090\n",
            "Epoch 9270/10000, Loss: -2.0093\n",
            "Epoch 9280/10000, Loss: -2.0094\n",
            "Epoch 9290/10000, Loss: -2.0092\n",
            "Epoch 9300/10000, Loss: -2.0097\n",
            "Epoch 9310/10000, Loss: -2.0096\n",
            "Epoch 9320/10000, Loss: -2.0095\n",
            "Epoch 9330/10000, Loss: -2.0085\n",
            "Epoch 9340/10000, Loss: -2.0091\n",
            "Epoch 9350/10000, Loss: -2.0094\n",
            "Epoch 9360/10000, Loss: -2.0098\n",
            "Epoch 9370/10000, Loss: -2.0093\n",
            "Epoch 9380/10000, Loss: -2.0089\n",
            "Epoch 9390/10000, Loss: -2.0094\n",
            "Epoch 9400/10000, Loss: -2.0098\n",
            "Epoch 9410/10000, Loss: -2.0094\n",
            "Epoch 9420/10000, Loss: -2.0098\n",
            "Epoch 9430/10000, Loss: -2.0100\n",
            "Epoch 9440/10000, Loss: -2.0093\n",
            "Epoch 9450/10000, Loss: -2.0092\n",
            "Epoch 9460/10000, Loss: -2.0091\n",
            "Epoch 9470/10000, Loss: -2.0094\n",
            "Epoch 9480/10000, Loss: -2.0094\n",
            "Epoch 9490/10000, Loss: -2.0098\n",
            "Epoch 9500/10000, Loss: -2.0096\n",
            "Epoch 9510/10000, Loss: -2.0094\n",
            "Epoch 9520/10000, Loss: -2.0099\n",
            "Epoch 9530/10000, Loss: -2.0102\n",
            "Epoch 9540/10000, Loss: -2.0090\n",
            "Epoch 9550/10000, Loss: -2.0099\n",
            "Epoch 9560/10000, Loss: -2.0095\n",
            "Epoch 9570/10000, Loss: -2.0099\n",
            "Epoch 9580/10000, Loss: -2.0097\n",
            "Epoch 9590/10000, Loss: -2.0098\n",
            "Epoch 9600/10000, Loss: -2.0098\n",
            "Epoch 9610/10000, Loss: -2.0092\n",
            "Epoch 9620/10000, Loss: -2.0100\n",
            "Epoch 9630/10000, Loss: -2.0101\n",
            "Epoch 9640/10000, Loss: -2.0096\n",
            "Epoch 9650/10000, Loss: -2.0100\n",
            "Epoch 9660/10000, Loss: -2.0099\n",
            "Epoch 9670/10000, Loss: -2.0096\n",
            "Epoch 9680/10000, Loss: -2.0093\n",
            "Epoch 9690/10000, Loss: -2.0095\n",
            "Epoch 9700/10000, Loss: -2.0094\n",
            "Epoch 9710/10000, Loss: -2.0097\n",
            "Epoch 9720/10000, Loss: -2.0097\n",
            "Epoch 9730/10000, Loss: -2.0096\n",
            "Epoch 9740/10000, Loss: -2.0094\n",
            "Epoch 9750/10000, Loss: -2.0097\n",
            "Epoch 9760/10000, Loss: -2.0097\n",
            "Epoch 9770/10000, Loss: -2.0098\n",
            "Epoch 9780/10000, Loss: -2.0104\n",
            "Epoch 9790/10000, Loss: -2.0095\n",
            "Epoch 9800/10000, Loss: -2.0099\n",
            "Epoch 9810/10000, Loss: -2.0095\n",
            "Epoch 9820/10000, Loss: -2.0100\n",
            "Epoch 9830/10000, Loss: -2.0092\n",
            "Epoch 9840/10000, Loss: -2.0101\n",
            "Epoch 9850/10000, Loss: -2.0105\n",
            "Epoch 9860/10000, Loss: -2.0100\n",
            "Epoch 9870/10000, Loss: -2.0102\n",
            "Epoch 9880/10000, Loss: -2.0095\n",
            "Epoch 9890/10000, Loss: -2.0103\n",
            "Epoch 9900/10000, Loss: -2.0095\n",
            "Epoch 9910/10000, Loss: -2.0090\n",
            "Epoch 9920/10000, Loss: -2.0097\n",
            "Epoch 9930/10000, Loss: -2.0098\n",
            "Epoch 9940/10000, Loss: -2.0097\n",
            "Epoch 9950/10000, Loss: -2.0096\n",
            "Epoch 9960/10000, Loss: -2.0102\n",
            "Epoch 9970/10000, Loss: -2.0101\n",
            "Epoch 9980/10000, Loss: -2.0105\n",
            "Epoch 9990/10000, Loss: -2.0093\n",
            "Epoch 10000/10000, Loss: -2.0102\n",
            "Total Bets: 2014, Total Profit: 2008.24, ROI: 99.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that we had a *return on investment* by almost **100%** ! We almost **doubled** the amount we started with by betting on 2014 matches ! The total profit means that, if we bet 1\\$ on every match, we'd end up with a profit of more than 2000\\$."
      ],
      "metadata": {
        "id": "xNokl2baLj34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the data contained just *10,000 matches*, we have to take this as a **win**, because as we've seen in the previous step (EDA), it's not that easy to find a **profitable strategy**."
      ],
      "metadata": {
        "id": "zOmsAGCBMlF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Saving the model**"
      ],
      "metadata": {
        "id": "U4sjxrsTLVPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'epoch': 10000,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': -2.0102,\n",
        "}, 'checkpoint.pth')\n"
      ],
      "metadata": {
        "id": "DCrY358P62b2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "Rb3qwT20J1tn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}